{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "import warnings\n",
    "import sklearn as sk\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import concurrent.futures\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout,\\\n",
    "                                    TimeDistributed, Conv1D, MaxPooling1D,\\\n",
    "                                    Flatten, Bidirectional, Input, Flatten,\\\n",
    "                                    Activation, Reshape, RepeatVector, Concatenate\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "\n",
    "train_dataset = pd.read_csv('inputs/train.csv',index_col='cfips')\n",
    "revealed_data = pd.read_csv('inputs/revealed_test.csv',index_col='cfips')\n",
    "\n",
    "cfips = train_dataset.index.unique()\n",
    "\n",
    "new_df = pd.DataFrame(columns=train_dataset.columns).astype(train_dataset.dtypes)\n",
    "\n",
    "for cfip in cfips: \n",
    "    new_df = pd.concat([new_df,train_dataset.loc[cfip], revealed_data.loc[cfip]])\n",
    "\n",
    "train_dataset = new_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.index = train_dataset.index.astype(\"string\")\n",
    "train_dataset['first_day_of_month'] = pd.to_datetime(train_dataset.first_day_of_month, format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 128535, 41)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of rows for each county \n",
    "cfips = train_dataset.index.unique()\n",
    "cfip = cfips[0]\n",
    "total_counties = len(cfips)\n",
    "len(train_dataset)/len(train_dataset.loc[cfip]) == total_counties, len(train_dataset), len(train_dataset.loc[cfip])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Outliers detection in population and MBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[\"population\"] = train_dataset[\"microbusiness_density\"]\n",
    "train_dataset[\"population\"] = 100*train_dataset[\"active\"]/train_dataset[\"microbusiness_density\"]\n",
    "train_dataset[\"population\"] = train_dataset[\"population\"].ffill()\n",
    "train_dataset[\"population\"] = train_dataset[\"population\"].apply(lambda x : int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking counties with abnormal variation in population\n",
    "\n",
    "def is_pop_outlier(cfip):\n",
    "    flag = False\n",
    "    data = train_dataset.loc[cfip].population.values\n",
    "\n",
    "    # calculate the Z-score for each data point\n",
    "    z_scores = np.abs((data - np.mean(data)) / np.std(data))\n",
    "    \n",
    "    # identify any data points with a Z-score greater than 3 (considered outliers)\n",
    "    outliers = data[z_scores > 3]\n",
    "    \n",
    "    if len(outliers)>0: \n",
    "        print(f\"cfip -> {cfip} : Found {len(outliers)} outliers.\")\n",
    "        flag = True\n",
    "    return flag\n",
    "    \n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    futures = [executor.submit(is_pop_outlier,cfip) for cfip in cfips]\n",
    "    results = [f.result() for f in concurrent.futures.as_completed(futures)]\n",
    "    \n",
    "print(\"number of counties with outliers:\" , sum(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.loc[\"28055\"].tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming counties with low population will have large variation in MBD on even small change in `active` \n",
    "# business , we will only consider counties having atleast population = POPULATION_THRESHOLD\n",
    "\n",
    "# Filtering counties with low population\n",
    "POPULATION_THRESHOLD = 5000        \n",
    "\n",
    "def above_threshold(cfip):\n",
    "    return train_dataset.loc[cfip].population.mean()>POPULATION_THRESHOLD\n",
    "\n",
    "low_population_counties = [cfip for cfip in cfips if not above_threshold(cfip)]\n",
    "high_population_counties = [cfip for cfip in cfips if cfip not in low_population_counties]\n",
    "print(\"Number of counties with low population : \", len(low_population_counties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null valus \n",
    "train_dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking counties with abnormal variation in MBD and fix them by substituting them with mbd values with \n",
    "# previous month\n",
    "\n",
    "def is_mbd_outlier(cfip,fix=False):\n",
    "    data = train_dataset.loc[cfip].microbusiness_density.values\n",
    "    # calculate the Z-score for each data point\n",
    "    z_scores = np.abs((data - np.mean(data)) / np.std(data))\n",
    "    \n",
    "    # identify any data points with a Z-score greater than 3 (considered outliers)\n",
    "    outliers = data[z_scores > 3]\n",
    "    \n",
    "    if len(outliers)>0: \n",
    "        # print(f\"cfip -> {cfip} : Found {len(outliers)} outliers.\")\n",
    "        \n",
    "        # fix the outliers by replacing them with the their previous month values\n",
    "        if fix: \n",
    "            flags = z_scores>3\n",
    "            for i in range(1,len(data)): \n",
    "                if flags[i] : \n",
    "                    data[i] = data[i-1]\n",
    "        \n",
    "            # data[z_scores>3] = np.median(data)  # alternate way if above doesn't work \n",
    "            \n",
    "            train_dataset.loc[cfip,\"microbusiness_density\"] = data\n",
    "        return 1\n",
    "    return 0\n",
    "    \n",
    "results = []\n",
    "for cfip in cfips:   \n",
    "    results.append(is_mbd_outlier(cfip,fix=True))\n",
    "print(\"number of counties with outliers in mbd (before fixing):\" , sum(results))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Utils Functions and Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 8\n",
    "output_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def split_data(data, input_size, output_size):\n",
    "    \"\"\"\n",
    "    Splits the time-series data into train, validation, and test sets, with a window size of input_size as the input\n",
    "    and output_size as the output.\n",
    "    \"\"\"\n",
    "    # MinMax Scaler\n",
    "    # scaler = MinMaxScaler()\n",
    "    scaler = StandardScaler()\n",
    "    data = scaler.fit_transform(data.values.reshape(-1,1))\n",
    "    # Calculate the total number of windows\n",
    "    num_windows = len(data) - input_size - output_size + 1\n",
    "    \n",
    "    # Split the data into input windows and output windows\n",
    "    inputs = np.zeros((num_windows, input_size,1))\n",
    "    outputs = np.zeros((num_windows, output_size,1))\n",
    "    \n",
    "    for i in range(num_windows):\n",
    "        inputs[i] = np.array(data[i:i+input_size]).reshape(-1,1)\n",
    "        outputs[i] = np.array(data[i+input_size:i+input_size+output_size]).reshape(-1,1)\n",
    "        \n",
    "    # Split the data into train, validation, and test sets\n",
    "    num_train = int(0.8 * num_windows)\n",
    "    num_val = num_windows - num_train\n",
    "    num_test = num_windows - num_train - num_val  # test size = 0 \n",
    "    \n",
    "    train_inputs = inputs[:num_train]\n",
    "    train_outputs = outputs[:num_train]\n",
    "    \n",
    "    val_inputs = inputs[num_train:num_train+num_val]\n",
    "    val_outputs = outputs[num_train:num_train+num_val]\n",
    "    \n",
    "    test_inputs = inputs[num_train+num_val:]\n",
    "    test_outputs = outputs[num_train+num_val:]\n",
    "    \n",
    "    prediction_inputs = np.array(data[-input_size:]).reshape(1,-1,1)\n",
    "    return train_inputs, train_outputs, val_inputs, val_outputs, \\\n",
    "           test_inputs, test_outputs, prediction_inputs, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function : smape ( symmetric mean absolute percentage error )\n",
    "def smape(A,F): \n",
    "    \"\"\"\n",
    "    calculate SMAPE ( Symmetric Mean Absolute Percentage Error) value\n",
    "    \n",
    "    Inputs: \n",
    "        A - actual value(s)\n",
    "        F - forecast value(s) \n",
    "    \"\"\"  \n",
    "    if type(A)==np.ndarray and type(F)==np.ndarray: \n",
    "        if(len(A) != len(F)): \n",
    "            print(\"Shapes didn't matched\")\n",
    "            return \n",
    "    a = np.abs(A-F)\n",
    "    b = (np.abs(A) + np.abs(F))/2\n",
    "    if type(A)==np.ndarray and type(F)==np.ndarray: \n",
    "        result = np.divide(a,b, out=np.zeros_like(a,dtype=type(a)), where=b!=0)\n",
    "        result = np.sum(result,axis=0)/len(result)\n",
    "    else: \n",
    "        result = a/b if b!=0 else np.zeros(1)\n",
    "    return result "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Building Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks \n",
    "\n",
    "# Model checkpoint - To save the model at certain frequencies \n",
    "checkpoint_filepath = \"./saved_models/checkpoint\"\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                                        filepath=checkpoint_filepath,\n",
    "                                        save_weights_only=False,\n",
    "                                        monitor=\"val_loss\",\n",
    "                                        mode=\"min\",\n",
    "                                        save_best_only=True)\n",
    "\n",
    "\n",
    "# Early stopping - stopping evaluation if monitored evaluation metric is no longer improving\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "                                            monitor=\"val_loss\",\n",
    "                                            min_delta=0.005,\n",
    "                                            patience=10,\n",
    "                                            mode=\"min\")\n",
    "\n",
    "# Reduce learning rate - decreasing the learning rate when the monitored metric has stopped improving\n",
    "rlrop_callback = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                                            monitor=\"val_loss\", \n",
    "                                            factor=0.2, \n",
    "                                            mode=\"min\", \n",
    "                                            patience=3, \n",
    "                                            min_lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model that takes this time series and output future predictions\n",
    "\n",
    "# Model Artchitecture - An encoder-decoder model \n",
    "# (source - https://towardsdatascience.com/cnn-lstm-based-models-for-multiple-parallel-input-and-multi-step-forecast-6fe2172f7668)\n",
    "\n",
    "def create_model1():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(GRU(70,activation=\"relu\",input_shape=(input_size,1)))\n",
    "    model.add(RepeatVector(output_size))\n",
    "    model.add(GRU(70,activation=\"relu\",return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(1)))\n",
    "    model.compile(optimizer=\"adam\",loss=\"mse\")\n",
    "    # plot_model(model=model,show_shapes=True)\n",
    "    return model \n",
    "\n",
    "def create_model2():\n",
    "\n",
    "    # Define the model architecture\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add the CNN layers \n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(input_size, 1)))\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Add the LSTM layers\n",
    "    model.add(LSTM(units=128, activation='relu', return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(units=64, activation='relu', return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(GRU(units=32, activation='relu'))\n",
    "\n",
    "    # Add the output layer\n",
    "    model.add(Dense(units=output_size))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return model \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform(scaler, y_test, yhat):\n",
    " y_test_reshaped = y_test.reshape(-1, y_test.shape[1])\n",
    " yhat_reshaped = yhat.reshape(-1, yhat.shape[1])\n",
    " yhat_inverse = scaler.inverse_transform(yhat_reshaped)\n",
    " y_test_inverse = scaler.inverse_transform(y_test_reshaped)\n",
    " return yhat_inverse, y_test_inverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results.csv','w') as fp: \n",
    "    fp.write('cfips,val_smape,pred_1,pred_2\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Train Model and Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "def run_model(cfip,model=None):\n",
    "    if not model : \n",
    "        model = create_model2()\n",
    "    \n",
    "    train_inputs, train_outputs, val_inputs, val_outputs, test_inputs, test_outputs, prediction_inputs,scaler = \\\n",
    "        split_data(train_dataset.loc[cfip].microbusiness_density, input_size, output_size)\n",
    "        \n",
    "    model.fit(train_inputs,train_outputs,\n",
    "            epochs=epochs,batch_size=batch_size, \n",
    "            validation_data=(val_inputs,val_outputs),\n",
    "            callbacks=[early_stopping_callback, rlrop_callback],\n",
    "            verbose=0)    \n",
    "    model.verbose = False\n",
    "    predictions = model.predict(val_inputs,verbose=0)\n",
    "    predictions, val_outputs = inverse_transform(scaler,predictions,val_outputs)\n",
    "    val_smape = np.mean(smape(predictions,val_outputs))\n",
    "    # future_predictions = scaler.inverse_transform(model.predict(prediction_inputs,verbose=0))\n",
    "    future_predictions = list(scaler.inverse_transform(model.predict(prediction_inputs,verbose=0).reshape(-1,output_size)).reshape(-1))\n",
    "    \n",
    "    with open('results.csv','a') as fp : \n",
    "        fp.write('{},{}\\n'.format(cfip,val_smape))\n",
    "    return cfip, val_smape, future_predictions\n",
    "    \n",
    "# with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "#     subset = cfips[:20]\n",
    "#     results = list(tqdm(executor.map(run_model,subset),total=len(subset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Submit predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the output to submission_file.csv\n",
    "\n",
    "# def submit():\n",
    "df = pd.read_csv('results.csv')\n",
    "submission = pd.read_csv('inputs/sample_submission.csv')\n",
    "submission['cfips'] = submission['row_id'].apply(lambda x : x.split('_')[0])\n",
    "df.cfips = df.cfips.astype('string')\n",
    "submission.cfips = submission.cfips.astype('string')\n",
    "submission = submission.set_index('cfips')\n",
    "df.set_index('cfips',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfips = submission.index.unique()\n",
    "for cfip in cfips: \n",
    "    submission.loc[cfip,\"microbusiness_density\"] = df.loc[cfip,\"pred_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"submission_2_march.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission_2_march.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[\"cfips\"] = submission[\"row_id\"].apply(lambda x : x.split(\"_\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.set_index(\"cfips\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.loc[\"1013\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) Ensemble Model - GRU + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the CNN-GRU model\n",
    "def cnn_lstm_model(n_input, n_output, n_filters, filter_size, n_lstm_units, n_lstm_layers, dropout_rate):\n",
    "    inputs = Input(shape=(n_input,1))\n",
    "\n",
    "    # CNN Layers\n",
    "    conv1 = Conv1D(filters=n_filters, kernel_size=filter_size, activation='relu')(inputs)\n",
    "    lstm_in = Dropout(rate=dropout_rate)(conv1)\n",
    "\n",
    "    # GRU Layers\n",
    "    lstm1 = GRU(units=n_lstm_units, return_sequences=True)(lstm_in)\n",
    "    for i in range(n_lstm_layers-1):\n",
    "        lstm1 = GRU(units=n_lstm_units, return_sequences=True)(lstm1)\n",
    "        lstm1 = Dropout(rate=dropout_rate)(lstm1)\n",
    "    lstm_out = GRU(units=n_lstm_units)(lstm1)\n",
    "\n",
    "    # Output Layer\n",
    "    outputs = Dense(n_output)(lstm_out)\n",
    "\n",
    "    # create and compile the model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# define the XGBoost model\n",
    "def xgb_model_create(n_estimators, max_depth):\n",
    "    model = xgb.XGBRegressor(n_estimators=n_estimators, max_depth=max_depth)\n",
    "    return model\n",
    "\n",
    "# define the combined model\n",
    "def cnn_lstm_xgb_model(n_input, n_output, n_filters, filter_size, n_lstm_units, n_lstm_layers, dropout_rate, n_estimators, max_depth):\n",
    "    # define the CNN-LSTM model\n",
    "    cnn_lstm = cnn_lstm_model(n_input, n_output, n_filters, filter_size, n_lstm_units, n_lstm_layers, dropout_rate)\n",
    "\n",
    "    # define the XGBoost model\n",
    "    xgb_model = xgb_model_create(n_estimators, max_depth)\n",
    "\n",
    "    # define the inputs\n",
    "    inputs = Input(shape=(n_input,1))\n",
    "\n",
    "    # pass the inputs through the CNN-LSTM model\n",
    "    cnn_lstm_out = cnn_lstm(inputs)\n",
    "\n",
    "    # pass the CNN-LSTM output through the XGBoost model\n",
    "    xgb_out = xgb_model(cnn_lstm_out)\n",
    "\n",
    "    # create and compile the final model\n",
    "    model = Model(inputs=inputs, outputs=xgb_out)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'XGBRegressor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/mahendra.kumar.19032/gru2.ipynb Cell 41\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bhpc.iitgoa.ac.in/home/mahendra.kumar.19032/gru2.ipynb#Y111sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m cnn_lstm_xgb_model(input_size,output_size,\u001b[39m64\u001b[39;49m,\u001b[39m2\u001b[39;49m,\u001b[39m50\u001b[39;49m,\u001b[39m5\u001b[39;49m,\u001b[39m0.2\u001b[39;49m,\u001b[39m30\u001b[39;49m,\u001b[39m5\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bhpc.iitgoa.ac.in/home/mahendra.kumar.19032/gru2.ipynb#Y111sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# run_model(\"1001\",model)\u001b[39;00m\n",
      "\u001b[1;32m/home/mahendra.kumar.19032/gru2.ipynb Cell 41\u001b[0m in \u001b[0;36mcnn_lstm_xgb_model\u001b[0;34m(n_input, n_output, n_filters, filter_size, n_lstm_units, n_lstm_layers, dropout_rate, n_estimators, max_depth)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhpc.iitgoa.ac.in/home/mahendra.kumar.19032/gru2.ipynb#Y111sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m cnn_lstm_out \u001b[39m=\u001b[39m cnn_lstm(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhpc.iitgoa.ac.in/home/mahendra.kumar.19032/gru2.ipynb#Y111sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39m# pass the CNN-LSTM output through the XGBoost model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bhpc.iitgoa.ac.in/home/mahendra.kumar.19032/gru2.ipynb#Y111sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m xgb_out \u001b[39m=\u001b[39m xgb_model(cnn_lstm_out)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhpc.iitgoa.ac.in/home/mahendra.kumar.19032/gru2.ipynb#Y111sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m# create and compile the final model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhpc.iitgoa.ac.in/home/mahendra.kumar.19032/gru2.ipynb#Y111sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m model \u001b[39m=\u001b[39m Model(inputs\u001b[39m=\u001b[39minputs, outputs\u001b[39m=\u001b[39mxgb_out)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'XGBRegressor' object is not callable"
     ]
    }
   ],
   "source": [
    "model = cnn_lstm_xgb_model(input_size,output_size,64,2,50,5,0.2,30,5)\n",
    "# run_model(\"1001\",model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "585a938ec471c889bf0cce0aed741a99eaf47ca09c0fa8393793bc5bfe77ba11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
